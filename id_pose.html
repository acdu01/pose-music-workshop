<div>Teachable Machine Pose Model</div>
<button type="button" onclick="init()">Start</button>
<div><canvas id="canvas"></canvas></div>
<div id="label-container"></div>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.3.1/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@teachablemachine/pose@0.8/dist/teachablemachine-pose.min.js"></script>
<script type="text/javascript">
    // More API functions here:
    // https://github.com/googlecreativelab/teachablemachine-community/tree/master/libraries/pose

    // the link to your model provided by Teachable Machine export panel
    // Model files are in this project root (model.json, metadata.json, weights.bin)
    const URL = "./";
    let model, webcam, ctx, labelContainer, maxPredictions;
    let audioCtx, lastPlayedPose = "";
    const CONFIDENCE_THRESHOLD = 0.85;

    const NOTE_FREQ = {
        A: 440.0,  // A4
        B: 493.88, // B4
        C: 261.63, // C4
        D: 293.66, // D4
        E: 329.63, // E4
        F: 349.23, // F4
        G: 392.0   // G4
    };

    async function init() {
        const modelURL = URL + "model.json";
        const metadataURL = URL + "metadata.json";

        // load the model and metadata
        // Refer to tmImage.loadFromFiles() in the API to support files from a file picker
        // Note: the pose library adds a tmPose object to your window (window.tmPose)
        model = await tmPose.load(modelURL, metadataURL);
        maxPredictions = model.getTotalClasses();

        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        await audioCtx.resume();

        // Convenience function to setup a webcam
        const size = 200;
        const flip = true; // whether to flip the webcam
        webcam = new tmPose.Webcam(size, size, flip); // width, height, flip
        await webcam.setup(); // request access to the webcam
        await webcam.play();
        window.requestAnimationFrame(loop);

        // append/get elements to the DOM
        const canvas = document.getElementById("canvas");
        canvas.width = size; canvas.height = size;
        ctx = canvas.getContext("2d");
        labelContainer = document.getElementById("label-container");
        for (let i = 0; i < maxPredictions; i++) { // and class labels
            labelContainer.appendChild(document.createElement("div"));
        }
    }

    async function loop(timestamp) {
        webcam.update(); // update the webcam frame
        await predict();
        window.requestAnimationFrame(loop);
    }

    async function predict() {
        // Prediction #1: run input through posenet
        // estimatePose can take in an image, video or canvas html element
        const { pose, posenetOutput } = await model.estimatePose(webcam.canvas);
        // Prediction 2: run input through teachable machine classification model
        const prediction = await model.predict(posenetOutput);

        for (let i = 0; i < maxPredictions; i++) {
            const classPrediction =
                prediction[i].className + ": " + prediction[i].probability.toFixed(2);
            labelContainer.childNodes[i].innerHTML = classPrediction;
        }

        let best = prediction[0];
        for (let i = 1; i < prediction.length; i++) {
            if (prediction[i].probability > best.probability) best = prediction[i];
        }

        const note = classToNote(best.className);
        const freq = NOTE_FREQ[note];
        if (freq && best.probability >= CONFIDENCE_THRESHOLD && best.className !== lastPlayedPose) {
            playNote(freq);
            lastPlayedPose = best.className;
        } else if (best.probability < CONFIDENCE_THRESHOLD) {
            // Re-arm when pose confidence drops so the next clear pose can trigger again.
            lastPlayedPose = "";
        }

        // finally draw the poses
        drawPose(pose);
    }

    function drawPose(pose) {
        if (webcam.canvas) {
            ctx.drawImage(webcam.canvas, 0, 0);
            // draw the keypoints and skeleton
            if (pose) {
                const minPartConfidence = 0.5;
                tmPose.drawKeypoints(pose.keypoints, minPartConfidence, ctx);
                tmPose.drawSkeleton(pose.keypoints, minPartConfidence, ctx);
            }
        }
    }

    function classToNote(className) {
        const cleaned = className.trim().toUpperCase();
        if (NOTE_FREQ[cleaned]) return cleaned;
        if (cleaned.startsWith("POSE ")) {
            const maybeNote = cleaned.slice(5).trim();
            if (NOTE_FREQ[maybeNote]) return maybeNote;
        }
        return null;
    }

    function playNote(freq) {
        if (!audioCtx) return;
        const osc = audioCtx.createOscillator();
        const gain = audioCtx.createGain();
        // Slightly richer than a pure sine, closer to a struck piano-like tone.
        osc.type = "triangle";
        osc.frequency.value = freq;

        const t0 = audioCtx.currentTime;
        gain.gain.cancelScheduledValues(t0);
        gain.gain.setValueAtTime(0.0001, t0);
        gain.gain.exponentialRampToValueAtTime(0.22, t0 + 0.008); // fast attack
        gain.gain.exponentialRampToValueAtTime(0.0001, t0 + 1.8); // longer decay

        osc.connect(gain);
        gain.connect(audioCtx.destination);
        osc.start();
        osc.stop(t0 + 1.85);
    }
</script>
